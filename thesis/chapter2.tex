\chapter{Использование методов оффлайн дизайна в системах управления с прогнозирующей моделью }

Метод MPC опирается на решение задач ОУ в режиме реального времени, однако несмотря на то, что существуют методы эффективного решения этих задач \cite{CasADi, Diehl} и развитие вычислительной техники позволяет быстро решать достаточно сложные задачи, для многих классов нелинейных систем существующие методы могут могут оказаться неэффективными или слишком медленными. Для преодоления этих недостатков в магистерской диссертации предлагается вынести некоторые вычисления из классического алгоритма MPC оффлайн. В данной главе будут рассмотрены основные существующие подходы для задач MPC, а также теория методов машинного обучения, которая будет использоваться в экспериментах в рамках магистерской диссертации.

\section{Основные понятия нейронных сетей}

Нейронная сеть представляет собой серию алгоритмов, которые стремятся распознать базовые отношения в наборе данных посредством процесса, который имитирует работу человеческого мозга.

Нейронная сеть основана на наборе связанных единиц или узлов, называемых искусственными нейронами, которые свободно моделируют нейроны в биологическом мозге. Каждое соединение, подобно синапсам в биологическом мозге, может передавать сигнал от одного искусственного нейрона к другому. Искусственный нейрон, который получает сигнал, может обрабатывать его, а затем сигнализировать дополнительные искусственные нейроны, связанные с ним \cite{Goodfellow}.

Ключевой моделью глубокого обучения являются нейронные сети с прямым распространением (многослойные персептроны). Целью данного вида нейронных сетей является аппроксимация некоторой функции $f^*$. Например, для классификатора $y = f^*(x)$ сеть отображает вход $x$ в категорию $y$. Сеть определяет отображение $y = f(x; \theta)$ и изучает значение параметров $\theta$, которые приводят к приближению функции наилучшим образом.

\begin{figure}[h]
\center{\includegraphics[scale=0.7]{neural_networks}}
\caption{Нейронная сеть со скрытыми слоями.}
\label{fig:nn}
\end{figure}

Нейронные сети называются сетями, потому что они обычно представляются объединением многих различных функций. Модель связана с ориентированным ациклическим графом (Рис. \ref{fig:nn}), описывающим, как функции состоят вместе. Например, мы могли бы иметь три функции $f^{(1)}$, $f^{(2)}$ и $f^{(3)}$, связанные в цепочке, с образованием $f(x) = f^{(3)}(f^{(2)}(f^{(1)}(x)))$. Эти цепные структуры являются наиболее часто используемыми структурами нейронных сетей. В этом случае $f^{(1)}$ называется первым слоем сети, $f^{(2)}$ называется вторым слоем и т. д. Длина цепочки слоев называется глубиной сети.

\begin{figure}[h]
\center{\includegraphics[scale=0.7]{one_neuron}}
\caption{Строение нейрона.}
\label{fig:one_neuron}
\end{figure}

Нейрон обычно получает много одновременных входов. Каждый вход имеет свой собственный относительный вес, который дает входное воздействие, которое ему необходимо для функции суммирования элемента обработки. Эти веса выполняют тот же тип функции, что и различные синаптические силы биологических нейронов. Веса --- это адаптивные коэффициенты в сети, которые определяют интенсивность входного сигнала, зарегистрированного искусственным нейроном. На рисунке (\ref{fig:one_neuron}) веса обозначены $w_i$, значения нейронов предыдущего слоя --- $a_i$. $b$ параметр представляет собой смещение для линейного преобразования входных нейронов. Таким образом, мы получаем значение функции суммирования в виде линейного преобразования $z = b + \sum_{i=1}^{N}a_iw_i$.

Функция $g$ на рисунке (\ref{fig:one_neuron}) - это функция активации нейрона.  Цель использования функции активации заключается в том, чтобы позволить суммируемому результату меняться в зависимости от времени.Функцией по умолчанию является нелинейная активационная функция ReLU $g(x) = max(0,x)$,которая рекомендованна для использования с большинством нейронных сетей прямого распространения. Применение этой функции к выходу линейного преобразования приводит к нелинейному преобразованию. Поскольку ReLU почти линейна, она сохраняет многие свойства, которые упрощают оптимизацию линейных моделей с помощью методов, основанных на градиенте. Другими популярными видами функции активации являются сигмоидальная(логистическая) функция $\sigma(x)=\frac{1}{1+e^{-x}}$, гиперболический тангенс $tanh(x) = \frac{e^x -e^{-x}}{e^x + e^{-x}}$.

Существует несколько видов обучения нейронных сетей: обучение с учителем и обучение без учителя.

Подавляющее большинство искусственных нейронных сетевых решений проходят обучение с учителем. В этом режиме фактический выход нейронной сети сравнивается с желаемым выходом. Веса, которые обычно начинаются с произвольного начала, затем корректируются сетью, так что следующая итерация или цикл приведут к более близкому совпадению между желаемым и фактическим выходом. Метод обучения пытается минимизировать текущие ошибки всех элементов обработки. Это глобальное сокращение ошибок создается со временем, постоянно изменяя весы ввода до тех пор, пока не будет достигнута приемлемая точность сети.

Обучение состоит в представлении входных и выходных данных в сеть. Эти данные часто упоминаются как тренировочный набор. То есть для каждого набора входных данных, предоставляемого системе, также предусмотрен соответствующий желаемый выходной набор. Затем используя метрики для расчета точности и качества модели, происходит процесс тренировки сети. Когда процесс тренировки заканчивается, то уже в онлайн процессах используются эти натренированные параметры и веса. 

Сети без учителя не используют внешние воздействия для корректировки своих весов. Вместо этого они внутренне контролируют свою работу. Эти сети ищут закономерности или тенденции во входных сигналах и делают адаптацию в соответствии с функцией сети. Хотя и сеть обучается сама, необходимо специализировать, как сети огранизовать себя. Эта информация встроена в сетевую топологию и правила обучения.

Алгоритм обучения - алгорим обратного распространения ошибки, в котором используется стохастический градиентный спуск. Для задачи регресии чаще всего в качестве функции потерь используется средняя квадратичная ошибка(MSE)(\ref{loss_function}): 
\begin{equation}\label{loss_function}
L(y_{out},y_{true}) = \frac{1}{N}\sum_{i=1}^N(y_{out}(i) -y_{true}(i))^2
\end{equation}

Согласно универсальной теореме аппроксимации — нейронная сеть с одним скрытым слоем может аппроксимировать любую непрерывную функцию многих переменных с любой точностью. Необходимо определить достаточное количество нейронов для достижения этой точности. С любой нелинейностью сеть остаётся универсальным аппроксиматором и при правильном выборе структуры может достаточно точно аппроксимировать функционирование любой непрерывной функции.

\section{Методы обучения c подкреплением}

Обучение с подкреплением (RL) - это область машинного обучения, связанная с тем, как агенты, взаимодейтсвующие со средой, должны предпринимать действия в этой среде, чтобы максимизировать некоторое понятие кумулятивной награды. В литературе по исследованиям и контролю операций обучение с подкреплением называется приближенным динамическим программированием или нейродинамическим программированием. Интерес к обучению с подкреплением возник также в теории оптимального управления, которая в основном связана с существованием и характеристикой оптимальных решений, алгоритмами их точного вычисления или аппроксимацией, особенно в отсутствие математической модели среды.

В машинном обучении среда обычно формулируется как процесс принятия решений Маркова (MDP), так как многие алгоритмы обучения c подкреплением для этого контекста используют методы динамического программирования. Основное различие между классическими методами динамического программирования и алгоритмами обучения с подкреплением заключается в том, что последние не предполагают знания точной математической модели MDP и нацеливаются на большие MDP, где точные методы становятся неосуществимыми. Компромисс между использованием наилучшей вычисленной стратегии и исследованием новых стратегий наиболее тщательно изучается в рамках многорукой бандитской проблемы и в конечных MDP.

Базовое RL моделируется как процесс принятия марковских решений:
\begin{itemize}
\item набор состояний среды и агента $S$
\item набор действий агента $A$
\item $P_{a}(s,s')=Pr(s_{t+1}=s'|s_{t}=s,a_{t}=a)$ - вероятность перехода из состояния $s$ в состояние $s'$ под действием агента $a$.
\item $R_ {a}(s, s')$ - непосредственная награда после перехода от $s$ к $s'$ с действием $a$.
\end{itemize}

Обучение с подкреплением требует механизмов исследования. Случайный выбор действий без ссылки на вероятное распределение вероятности показывает низкую производительность. Простые методы исследования являются наиболее практичными.

Одним из таких методов является $\epsilon$ -greedy, когда агент выбирает действие, которое, по его мнению, имеет лучший долгосрочный эффект с вероятностью $1- \epsilon$. Если никакое действие, удовлетворяющее этому условию, не найдено, агент выбирает действие равномерно случайным образом. Здесь $\epsilon <1$ является параметром настройки, который иногда изменяется либо по фиксированному расписанию, либо адаптивно основываясь на эвристике.

Выбор действия агента моделируется как таблица, называемая стратегией $\pi: S \times A \to [0,1]$:
\begin{equation}
\pi (a | s) = P (a_{t} = a | s_{t} = s).
\end{equation}
Таблица стратегии дает вероятность принятия действий $a$ в состоянии $s$. Существуют также невероятностные стратегии.

Функция стоимости $V_{\ pi}(s)$ определяется как ожидаемый возврат, начиная с состояния $s$, то есть $s_ {0} = s$ и последовательно следуют политике $\pi$. Следовательно, грубо говоря, функция значений оценивает «насколько хорошо» она должна находиться в определенном состоянии. 
\begin{equation}
V_{\pi}(s) = E[R] = E[\sum_{t = 0}^{\infty} \gamma^{t} r_{t} | s_{0} = s].
\end{equation}

где случайная величина $R$ обозначает прибыль, и определяется как сумма будущих дисконтированных вознаграждений.
\begin{equation}
R = \sum_{t = 0}^{\infty} \gamma ^ {t} r_ {t},
\end{equation}
где $r_ {t}$ - вознаграждение на этапе $t$,$\gamma \in [0,1]$ --- коэффициент дисконтирования.

Функция стоимости пытается найти стратегию, которая максимизирует прибыль, поддерживая набор оценок ожидаемых результатов для некоторой стратегии (обычно либо «текущой» [внутри стратегии], либо оптимальной [вне стратегии]).

На основании теории MDP, стратегия называется оптимальной, если она достигает наилучшей ожидаемой прибыли из любого начального состояния. 

Чтобы определить оптимальность формальным образом, определим прибыль стратегии $\pi$
\begin{equation}
 V^{\pi} (s) = E[R | s, \pi ]
\end{equation}
где $R$ обозначает прибыль, связанную со следующей $\pi$ из исходного состояния $s$. Определим $V^{*}(s)$ как максимально возможное значение $V^{\pi}(s)$, где $\pi$ разрешено изменять,
\begin{equation}
V^{*}(s) = \max_{\pi} V^{\pi}(s)
\end{equation}
Стратегия, которая достигает этих оптимальных значений в каждом состоянии, называется оптимальной. 

Хотя значения состояний достаточно для определения оптимальности, полезно определить функцию прибыли от действия агентв. Учитывая состояние $s$, действие $a$ и политику $\pi$, значение действия пары $(s, a )$ от $\pi$ определяется формулой
\begin{equation}
Q^{\pi} (s, a) = E [R | s, a, \pi]
\end{equation}
где $R$ теперь обозначает случайную прибыль, связанную с первым действием $a$ в состоянии $s$ и последующим $\pi$.

Теория MDP утверждает, что если $\pi^*$ является оптимальной стратегией, мы принимаем оптимальное действие, выбирая его из $Q^{\pi^{*}} (s, \cdot)$ с наивысшим значением в каждом состоянии, $s$. Функция прибыли от действия такой оптимальной стратегией $ Q^{\pi^{*}}$ называется оптимальной функцией прибыли от действия и является обычно обозначаемый $Q^{*}$. Таким образом, знание оптимальной функции стоимости действия достаточно, чтобы знать, как действовать оптимально.

Существует несколько алгоритмов для нахождения оптимальных стратегий: метод Монте-Карло, метод конечных разностей, метод прямого поиска стратегии. Каждый из которых имеет свои достоинства и недостатки, однако чаще всего используется стохастическая оптимизация и методы градиентного подъема.

\section{Методы дизайна оффлайн регуляторов в системах управления с прогнозирующей моделью}

Существует несколько подходов использования методов обучения в MPC системах:
\begin{itemize}
\item Явный MPC
\item Аппроксимация закона управления 
\item Использование обучаемой модели для аппроксимации динамики прогнозирующей модели
\item Итерационный подход для построения терминального региона и функции из предыдущих итераций
\end{itemize}

\subsection{Явный MPC}
При некоторых слабых предположениях для линейных систем задача оптимизации может быть решена оффлайн, т.е. до начальной реальной процедуры управления \cite{Bemporad}. В результате, получается явный закон управления $u(x)$. Развитие подхода на нелинейные системы не тривиально, и кроме того даже в линейных случаях существуют проблемы с эффективностью вычислений для метода из \cite{Bemporad}.

В момент времени $t$, вычисляем состояние $x(t)$, решаем линейную задачу с линейными ограничениями
\begin{equation}\label{sys_lin_explicit}
V(x) = \min_{u(\cdot|t)} \sum_{k=t}^{t+N-1}L(x(k|t),u(k|t)) + F(x(t+N|t))
\end{equation}
при условиях
\begin{equation*}
x(k+1|t) = Ax(k|t) + Bu(k|t), \  t \leq k \leq t+N-1
\end{equation*}
\begin{equation*}
x(t|t) = x(t), \  t \leq k \leq t+N-1
\end{equation*}
\begin{equation*}
C_xx(k|t) \leq d_x, \  t \leq k \leq t+N-1
\end{equation*}
\begin{equation*}
C_uu(k|t) \leq d_u, \  t \leq k \leq t+N-1
\end{equation*}
\begin{equation*}
C^fx(t+N|t) \leq d^f,
\end{equation*}
с квадратичной функцией стоимости этапа и квадратичной терминальной функцией
\begin{equation*}
L(x(t),u(t)) = x(t)^TQx(t) + u(t)^TRu(t), \ Q,R > 0, \ F(x(t)) = x(t)^TPx(t)
\end{equation*}

Можно переписать задачу \ref{sys_lin_explicit} в виде задачи квадратичного программирования. Для этого обзначим
\begin{equation}
X:= [x^T(t+1|t), \dots, x^T(t+N|t)]^T,
\end{equation}
\begin{equation}
U:= [u^T(t+1|t), \dots, u^T(t+N-1|t)]^T,
\end{equation}

Перепишем функцию стоимости
	\begin{equation}\label{cost_function}
	F(x(t),U) = x^T(t)Qx(t) + X^T\tilde{Q}X + U^T\tilde{R}U,
	\end{equation}
	где $\tilde{Q} = diag(Q,..,Q,P) \in \mathbb{R}^{n \times (N+1)} $, 
	$\tilde{R} = diag(R,..,R) \in \mathbb{R}^{m \times N} $

Перепишем динамику системы: 
\begin{equation}
x(t+k|t) = A^kx(t) + \sum_{j=0}^{k-1}A^jBu(t+k-j-1|t), \ k=1,..,N,
\end{equation}
Тогда
	\begin{equation}\label{rewritten_state_input}
X=\begin{bmatrix}
   A \\
   A^2 \\
   \vdots \\
   A^N   
\end{bmatrix}x(t) +\begin{bmatrix}
    B  & 0 & \dots &  & 0 \\
    AB & B & 0 & \dots & 0\\
    \hdots & & & & \\
    A^{N-1}B  & A^{N-2}B & \dots & \dots & B     
\end{bmatrix}U
\end{equation}
или, введя обозначения $\Omega = \begin{bmatrix}
   A \\
   A^2 \\
   \vdots \\
   A^N   
\end{bmatrix}$ и $\Gamma = \begin{bmatrix}
    B  & 0 & \dots &  & 0 \\
    AB & B & 0 & \dots & 0\\
    \hdots & & & & \\
    A^{N-1}B  & A^{N-2}B & \dots & \dots & B     
\end{bmatrix}$
сократим запись $X = \Omega x(t) + \Gamma U$.

С помощью (\ref{rewritten_state_input}) перепишем (\ref{cost_function}) : 
\begin{equation*}
J(x(t),U) = \frac{1}{2}x^T(t)Yx(t) + \frac{1}{2}U^THU + x^T(t)FU
\end{equation*}
где
\begin{equation*}
Y = 2(Q + \Omega^T \tilde{Q} \Omega),
\end{equation*}
\begin{equation*}
H = 2(\Gamma^T\tilde{Q}\Gamma + \tilde{R}),
\end{equation*}
\begin{equation*}
F = 2\Omega^T \tilde{Q} \Gamma.
\end{equation*}

Аналогично, для всех ограничений можно провести такие же преобразования  
\begin{equation*}
GU \leq W + Ex(t)
\end{equation*}
где
\begin{equation*}
G = diag(C_x,...,C_x)\Gamma, \ W = [d_x,..,d_x]^T, \ E = diag(C_x,..,C_x)\Omega.
\end{equation*}

Тогда, задача (\ref{sys_lin_explicit}) примет вид
\begin{equation}\label{standard_problem}
\min_U \frac{1}{2} U^THU + x^TFU + \frac{1}{2}x^TYx,
\end{equation}
при условии 
\begin{equation}
GU \leq W + Ex(t)
\end{equation}

Применим замену переменных вида 
\begin{equation*}
z:=U+H^{-1}F^Tx,
\end{equation*}

Нетрудно заметить, что $H^{-1}$ --- положительно определенная матрица. Тогда (\ref{standard_problem}) примет вид 

\begin{equation}\label{rewritten_problem}
\min_{z} \frac{1}{2}z^THz + \frac{1}{2}x^T \tilde{Y}x,
\end{equation}
при условиях 
\begin{equation*}
Gz \leq W + Sx,
\end{equation*}
\begin{equation*}
\tilde{Y} := Y - FH^{-1}F^T,
\end{equation*}
\begin{equation*}
S := E + GH^{-1}F^T.
\end{equation*}

Далее эта задача решается с помощью теории выпуклой оптимизации через условия Каруша-Куна-Такера(ККТ). Так как задача (строго)выпуклая с допустимым множеством с непустой внутренней частью(по предположениям), то условия Слейтера выполняются. Оптимальное решение единственное и характеризуется условиями KKT.

Явный MPC решает задачу для всех состояний, таким образом все пространство состояний делится на области, где в каждой области для состояния есть явная функция управления. 

Алгоритм нахождения явных функций управления:
\begin{enumerate}
\item Взять любой $x_0 \in \mathbb{X}$
\item Решить задачу (\ref{rewritten_problem}) c начальным условием $x = x_0$
\item Определить активные ограничения для оптимизационной задачи (\ref{rewritten_problem}) 
\item Вычислить критическую область по активным ограничениям и вычислить функцию управления для этой области
\item Перейти к новому $x_0$
\end{enumerate}

Главный недостаток этого метода состоит в том, что количество областей может быть достаточно большим, что в онлайн процедуре может плохо сказываться на производительности. Так как в каждый момент времени нужно будет искать к какому из регионов относится текущее состояние, чтобы опредилить управление для него.

\subsection{Аппроксимация закона управления}

Cуществует несколько подходов к получению аппроксимативного решения для оптимизации MPC. Для линейных систем в \cite{Domahidi} алгоритм обучения представлен дополнительными ограничениями для обеспечения стабильности и ограничения удовлетворения аппроксимационного MPC. Одним из подходов к аппроксимации MPC является выпуклое многопараметрическое нелинейное программирование \cite{Johansen}, где вычисляется субоптимальная аппроксимация закона управления MPC. Другой подход - аппроксимировать MPC с помощью методов машинного обучения. Это делают нейронные сети в \cite{ParisiniZoppoli}, \cite{ParisiniSanquineti}, \cite{Akesson}. Эти методы не гарантируют устойчивость или удовлетворение ограничениям для аппроксимационного MPC, что особенно важно, если рассматривать жесткие ограничения на состояния. В \cite{Chakrabarty} используется метод опорных векторов (SVM) для аппроксимации MPC. Устойчивость и удовлетворение ограничений могут быть гарантированы для произвольных ошибок малого приближения, основанных на присущих свойствам устойчивости. В \cite{Pin} аппроксимируется MPC с липшицевым сужающим ограничением, что обеспечивает устойчивость при неисчезащих ошибках аппроксимации. Ошибка аппроксимации, выведенная в \cite{Chakrabarty}, \cite{Pin}, обычно не достижима для практического применения.

B \cite{ParisiniSanquineti} находят аппроксимирующий закон управления
\begin{equation*}
u^{0}(t)=\gamma^0(x(t)) \in U,
\end{equation*} где $u^{0}(t)$ - первый вектор последовательности управления, которая минимизирует стоимость 
\begin{equation*}
J(x(t),u(t)) = \sum_{k=t}^{t+N-1}l(x(k),u(k)) + a \|x(t+N)\|^2_P, \ t \geq 0.
\end{equation*}

Стоимость формируется из стоимости переходов на горизонте планирования длины $N$ и терминальной функции. Аппроксимация закона управления происходит с помощью нейронной сети: $m$  параллельных сетей с одним выходным параметром,состоящий из одного скрытого слоем с $v_j$ нейронами на скрытом слое для $j=1..m$ сети и линейными активационными функциями. 

Для каждой функции $\hat{\gamma}^{(v_j)}_{j}$ нужно найти количество нейронов $v_1^*, ..., v^*_m$, такое что 
\begin{equation}\label{neuron_inequality}
\min_{w_j}\max_{x_t \in X}|\gamma^0_{j}(x(t)) - \gamma^{(v_j)}_{j}(x(t),w_j)| \leq \frac{\epsilon}{\sqrt{m}}, \ j = 1, ..., m.
\end{equation}

Процедура нахождения количества нейронов представляется таким образом: для каждого $j$ увеличиваем $v_j$, пока (\ref{neuron_inequality}) не станет верным. Также приводится в статье теорема, в которой утверждается, что для любой функции управления $\gamma^0_{j}(x_t)$ число параметров, необходимых для достижения погрешности приближения $L_2$ или $L_{\infty}$ порядка $O(\frac{1}{v_j})$, равно $O(v_jn)$, которое растет линейно с размерностью $n$ вектора состояния.

В работе \cite{Pin} уже вводятся некоторые гарантии устойчивости метода на основании предположения о непрерывности по Липшицу функции динамики. Система представляется следующим образом:
\begin{equation*}
x(t+1) = f(x(t),u(t), \xi_t), \ t \geq 0 , \ x_0 = \tilde{x},
\end{equation*}

где $\xi_t \in \mathbb{R}^r$ - возмущение системы. Номинальная система вводится для дизайна управления таким образом:
\begin{equation*}
x_(t+1) = \hat{f}(x(t),u(t)) + d_t, \ t \geq 0 , \ x_0 = \tilde{x},
\end{equation*}
где $d_t = f(x(t),u(t),\xi_t) - \hat{f}(x(t),u(t))$.
Предполагают непрерывность по Липшицу для функции $\hat{f}$ относительно $x$ с константой $L_{f_x}$, а также относительно управления $u$ и предполагают,что существует функция $K$ класса, такая что 
\begin{equation*}
|\hat{f}(x(t),u(t)) - \hat{f}(x(t),u'(t))| \leq \eta_u(|u(t)-u'(t)|) \ \forall x(t) \in X,\ \forall (u(t),u'(t)) \in U^2.
\end{equation*}

Также вводится предположение об ограничености возмущения и то, что верно $|d_t| \leq \mu(|\xi_t|) \ t \geq 0$, а также $d_t \in D = B^m(\bar(d)),\ \bar{d} \in \mathbb{R}_{\geq 0} < \infty$. Предполагается, что для системы существует управление $k(x(t)) \in U$, которое является стабилизирующим относительно состояния.

Накладываются дополнительные ограничения на погрешности относительно состояния $q_t \in Q = B^m(\bar{q})$ и управления $v_t \in V = B^m(\bar{v})$, где погрешность аппроксимации состояния $q_t = \xi_{t} - x(t)$ и управления $v_t = k^*(x_t) - k^*(\xi_{t})$. Тогда система уже переписывается в таком виде:
\begin{equation*}
x(t+1) = \hat{f}(x(t), k(x(t) + q_t)+v_t)+w_t, \ x_0 = \tilde{x}, \ t \geq 0.
\end{equation*} 

И устойчивость системы доказывается, если верно следующее,что погрешности аппроксимации состояния и управления совместено с возмущением ограничены изначальным возмущением системы $\bar{d_q} + \bar{d_v} + \bar{d_w} \leq \bar{d}$. В данной статье рассматривались два метода аппроксимации закона управления с помощью метода ближайшей точки и с помощью нейронной сети. Метод ближайшей точки обладает рядом недостатков, так как хранит целую сетку значений управления для состояний и еще требует в онлайн части решать задачу по нахождению ближайшей точки для текущего состояния. Нейронная сеть показала довольно хорошие результаты, так как она не обладает такими недостатками.

В работе с использованием результатов \cite{HertnerKohler} исследуются условия, при которых, несмотря на ошибки аппроксимации, гарантируется выполнение ограничений  и асимптотическая устойчивость замкнутой системы. На основе этого источника будет продолжаться исследование аппроксимации закона управления с помощью нейронной сети и результаты будут продемонстрированы в главе 3.

\subsection{Аппроксимация динамики системы с прогнозирующей моделью}

Существует методы для аппроксимации динамики системы, в работе \cite{Aswani} был предложен доказуемо безопасный и робастный метод управления, основанный на обучении для систем с прогнозирующей моделью, который назвается LBMPC. LBMPC изучает динамику системы по предоставленным точкам,также имеется возможность обновления динамики системы с помощью новых измерений, обеспечивает при этом безопасность и устойчивость, используя теорию из робастного MPC, чтобы проверить, примененное управление сохраняет ли номинальную модель устойчивой, когда она подвержена неопределенности.

Динамика системы представляется в таком виде:
\begin{equation*}
x(t+1) = Ax(t) + Bu(t) + g(x(t), u(t))
\end{equation*}
где $g(x(t),u(t))$ описывает несмоделированную динамику, которая по предположению ограничена и лежит в политопе $W$. 

Данный метод вводит дополнительную систему, которая обучается на данных и имеет такой вид:
\begin{equation*}
\tilde x(t+1) = A \tilde x(t) + B \tilde u(t) + O_n(\tilde x(t), \tilde u(t))
\end{equation*}
где $O_n$ - зависящая от времени функция, которая обучается с помощью любого из статистических методов. 

Вся теория устойчивости строится на том, что наша система представляется в виде робастного MPC с обученой динамикой на известных вычисленных точках, и может адаптироваться к новым полученным точкам, чтобы улучшать точность обученной динамики системы. 

Задача формулируется в таком виде:

\begin{equation*}
V_n(x(е)) = \min_{c, \theta} \phi_n(\theta, \tilde{x}(t), ..., \tilde{x}(t+N), \tilde{u}(t), ..., \tilde{u}(t+N-1))
\end{equation*}
при условиях
\begin{equation*}
\tilde{x}(t)=x_t, \ \bar{x}(t) = x_t,
\end{equation*}
\begin{equation*}
\tilde{x}(t+i+1) = A \tilde{x}(t+i) + B \tilde{u}(t+i) + O_n(\tilde{x}(t+i), \tilde{u}(t+i)),
\end{equation*}
\begin{equation*}
\bar{x}(t+i+1)=A\bar{x}(t+i)+B\tilde{u}(t+i),
\end{equation*}
\begin{equation*}
\tilde{u}(t+i) = K\bar{x}(t+i)+c_{n+i},
\end{equation*}
\begin{equation*}
\bar{x}(t+i+1) \in X \ominus R_i, \ \tilde{u}(t+i) \in U \ominus KR_i,
\end{equation*}
\begin{equation*}
(\bar{x}(t+N) \in \Omega \ominus (R_N \times \{0\}),
\end{equation*}

где $\Omega$ - допустимое инвариантное робастное множество таких точек, что любая траектория системы с начальным условием, выбранным из этого множества и с управлением $u(t)$, остается в множестве для любой последовательности ограниченного возмущения, удовлетворяя ограничениям на состояние и управление. 

Моделируется точка устойчивого состояния $\bar{x}_s = \Lambda \theta$ и управления $\bar{u}_s = \Psi \theta$, где $\theta \in \mathbb{R}^m$ и $\Lambda \in \mathbb{R}^{n \times m}, \ \Psi \in \mathbb{R}^{m \times m}$ - параметры моделирования, параметры будут описывать точку равновесия для системы, если $A+BK$ устойчива по Шуру при управлении 
\begin{equation*}
\bar{u}(t) = K(\bar{x}(t) - \bar{x}_s) + \bar{u}_s = K\bar{x}(t) + (\Psi - K\Lambda)\theta.
\end{equation*}
Множества $R_0 = \{0\}$ и $R_i = \oplus_{j=0}^{i-1}(A+BK)^jW$ необходимы для робастного MPC, они представляют собой сужающиеся ограничения. И тогда с помощью данного неравенства формулируется удовлетворение ограничений:
\begin{equation*}
\Omega \subseteq \{ (\bar{x}, \theta): \bar{x} \in X; \Lambda \theta \in X; K \bar{x} + (\Psi - K \Lambda)\theta \in U; \Psi \theta \in U\}.
\end{equation*}

А с помощью данного неравенства инвариантность возмущения:
\begin{equation*}
\begin{bmatrix}
   A+BK & B(\Psi - K\Lambda) \\
   0 &  \mathbb{I} 
\end{bmatrix}\Omega \oplus (W \times \{0\}) \subseteq \Omega.
\end{equation*}

Устойчивость данного метода основана на робастном MPC. Были получены хорошие результаты относительно предсказанных траекторий и быстрой сходимости к точке устойчивого состояния, однако по производительности уступало нелинейному MPC с нейросетевыми регуляторами.

\subsection{Итерационный подход}

Существует итерационный подход для построения MPC регулятора \cite{Rosolia}. Данный метод используется для повторяющихся задач, где эталонная траектория неизвестна. Например, системы для гоночных и раллийных машин, где среда и динамика сложны и не совсем известны. Регулятор имеет справочную информацию и способен улучшать свою эффективность, изучая предыдущие итерации. Для обеспечения рекурсивной выполнимости и неубывающей эффективности на каждой итерации используются безопасное терминальное множество и терминальная функция стоимости. Построение данного регулятора обеспечивает тот факт, что функция стоимости убывает с каждой итерацией, также из удовлетворения ограничений на $j-1$ итерации следует удовлетворение ограничений на $j$ итерации и точка равновесия замкнутой системы асимптотически устойчива. Оптимальность траекторий построенных этим регулятором доказывается для выпуклых задач.

\begin{figure}[h]
\center{\includegraphics[scale=0.7]{iterative}}
\caption{Построение сэмплированного безопасного множества.}
\label{fig:sample_safe_set}
\end{figure}

Управляемое на $N$ шагов вперед множество по отношению к $S$
\begin{equation}
K_j(S) = Pre(K_{j-1}(S)) \cap X, \ K_0(S) = S, j \in \{1, ..., N\}
\end{equation}
где $Pre(S) = \{x \in \mathbb{R}^n: \exists u \in U s.t.\ f(x,u) \in S\}$

Сэмплированное безопасное множество на итерации $j$
\begin{equation*}
SS^j = \{\cup_{i \in M^j} \cup_{t=0}^{\infty} x^i(t)\}
\end{equation*}
где $M^j = \{k \in [0,j]: \lim_{t \to \infty}x^k(t) = x_F \}$. 

$SS^j$ --- это множество всех траекторий на итерации $i$ для $i \in M^j$. Как выглядит это множество показано на рисунке \ref{fig:sample_safe_set}.

Cтоимость на сэмплированном безопасном множестве вводится таким образом, чтобы мы штрафовали те состояния, которые не находятся в нашем построенном безопасном множестве на предыдущих итерациях
\begin{equation*}
Q^j(x) = \left \{
  \begin{tabular}{l}
  $\min_{(i,t) \in F^j(x)} J^i_{t \to \infty}, \ x(t)\in SS^j$ \\
  $+ \infty, \ x \not\in SS^j$
  \end{tabular}
 \right.
\end{equation*}
где $\forall \ x(t) \in SS_j, \ Q_j(x) = J^{i^*}_{t^* \to \infty}(x(t)) = \sum_{k=t^*}^{\infty}
l(x^{i^*}(k) ,u^{i^*}(k) )$

\begin{equation*}
J_{t \to t+N}(x_t^j) = \min_{u_{t|t},..., u_{t+N-1|t}}[\sum_{k=t}^{t+N-1}l(x(k|t),u(k|t)) + Q^{j-1}(x(t+N|t))]
\end{equation*}
при условии
\begin{equation*}
x(k+1|t) = f(x(k|t), u(k|t)) \ \forall k \in [t, .., t+N-1]
\end{equation*}
\begin{equation*}
x(k|t) \in X, \ u(k|t) \in U, \ \forall k \in [t,..,t+N-1]
\end{equation*}
\begin{equation*}
x(t+N|t) \in SS^{j-1}
\end{equation*}
\begin{equation*}
x(t|t) = x^j(t)
\end{equation*}

Данный метод обладает рекурсивной выполнимостью, устойчивостью и сходимостью. Но предлагаемый подход является дорогостоящим с точки зрения вычисления даже для линейной системы, поскольку регулятор должен решать задачу смешанного целочисленного программирования в каждый момент времени. Есть улучшения данного подхода с использованием параллельных вычислений, а также попытки сделать терминальные ограничения более выпуклыми.

\section{Выводы}

Существует ряд подходов для вынесения вычислений оффлайн для систем с прогнозирующей моделью. Каждый метод используется для своего типа задач. Для линейных задач с небольшим количеством областей подходит явный MPC. Для задач с нелинейной динамикой будет более эффективным метод аппроксимации закона управления. Для задач с неизвестной динамикой, представленной в виде набора точек, лучше использовать аппроксимация динамики системы. Для задач с неизвестной динамикой или изменяющей средой полходят итеративные методы управления MPC.

В данной магистерской диссертации мы хотели вынести вычисления оффлайн для нелинейных систем,поэтому более эффективное направление --- использование аппроксимаци закона управления. Для этого будут использоваться нейронные сети, как универсальный аппроксиматор непрерывных функций.