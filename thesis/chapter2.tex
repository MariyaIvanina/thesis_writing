\chapter{Использование методов машинного обучения в системах управления с прогнозирующей моделью }

\section{Основные понятия нейронных сетей}

Нейронная сеть представляет собой серию алгоритмов, которые стремятся распознать базовые отношения в наборе данных посредством процесса, который имитирует работу человеческого мозга.

Нейронная сеть основана на наборе связанных единиц или узлов, называемых искусственными нейронами, которые свободно моделируют нейроны в биологическом мозге. Каждое соединение, подобно синапсам в биологическом мозге, может передавать сигнал от одного искусственного нейрона к другому. Искусственный нейрон, который получает сигнал, может обрабатывать его, а затем сигнализировать дополнительные искусственные нейроны, связанные с ним.

В общих реализациях нейронных сетей сигнал при соединении между искусственными нейронами является действительным числом, а выход каждого искусственного нейрона вычисляется некоторой нелинейной функцией от суммы его входов. Связи между искусственными нейронами называются ребрами. Искусственные нейроны и ребра обычно имеют вес, который регулируется по мере продолжения обучения. Вес увеличивает или уменьшает силу сигнала при соединении. Искусственные нейроны могут иметь такой порог, что сигнал посылается только тогда, когда совокупный сигнал пересекает этот порог. Как правило, искусственные нейроны агрегируются в слои. Различные слои могут выполнять различные виды преобразований на своих входах. Сигналы перемещаются от первого уровня (входного уровня) к последнему слою (выходному слою), возможно, после пересечения слоев несколько раз.

Ключевой моделью глубокого обучения являются нейронные сети с прямым распространением (многослойные персептроны). Целью данного вида нейронных сетей является аппроксимация некоторой функции $f^*$. Например, для классификатора $y = f^*(x)$ сеть отображает вход $x$ в категорию $y$. Сеть определяет отображение $y = f(x; \theta)$ и изучает значение параметров $\theta$, которые приводят к приближению функции наилучшим образом.

\begin{figure}[h]
\center{\includegraphics[scale=0.7]{neural_networks}}
\caption{Нейронная сеть со скрытыми слоями.}
\label{fig:nn}
\end{figure}

Нейронные сети называются сетями, потому что они обычно представляются объединением многих различных функций. Модель связана с ориентированным ациклическим графом (Рис. \ref{fig:nn}), описывающим, как функции состоят вместе. Например, мы могли бы иметь три функции $f^{(1)}$, $f^{(2)}$ и $f^{(3)}$, связанные в цепочке, с образованием $f(x) = f^{(3)}(f^{(2)}(f^{(1)}(x)))$. Эти цепные структуры являются наиболее часто используемыми структурами нейронных сетей. В этом случае $f^{(1)}$ называется первым слоем сети, $f^{(2)}$ называется вторым слоем и т. д. Длина цепочки слоев называется глубиной сети.

Каждый скрытый уровень сети обычно является векторным. Размерность этих скрытых слоев определяет ширину модели. Каждый элемент вектора может быть интерпретирован как играющий роль, аналогичную нейрону. Вместо того, чтобы думать о том, что слой представляет собой единую вектор-векторную функцию, мы также можем думать о том, что этот слой состоит из множества единиц, которые действуют параллельно, каждый из которых представляет собой вектор-скалярную функцию. Каждый блок напоминает нейрон в том смысле, что он получает вход от многих других единиц и вычисляет его собственное значение активации. 

\begin{figure}[h]
\center{\includegraphics[scale=0.7]{one_neuron}}
\caption{Строение нейрона.}
\label{fig:one_neuron}
\end{figure}

Нейрон обычно получает много одновременных входов. Каждый вход имеет свой собственный относительный вес, который дает входное воздействие, которое ему необходимо для функции суммирования элемента обработки. Эти веса выполняют тот же тип функции, что и различные синаптические силы биологических нейронов. В обоих случаях некоторые входы становятся более важными, чем другие, так что они оказывают большее влияние на обрабатывающий элемент, поскольку они объединяются для создания нейронного ответа. Веса - это адаптивные коэффициенты в сети, которые определяют интенсивность входного сигнала, зарегистрированного искусственным нейроном. Они являются мерой прочности соединения входа. Эти сильные стороны могут быть изменены в ответ на различные обучающие наборы и в соответствии с конкретной топологией сети или с помощью ее правил обучения.На рисунке (\ref{fig:one_neuron}) веса обозначены $w_i$, значения нейронов предыдущего слоя - $a_i$. $b$ параметр представляет собой смещение для линейного преобразования входных нейронов. Таким образом, мы получаем значение функции суммирования в виде линейного преобразования $z = b + \sum_{i=1}^{N}a_iw_i$.

Функция $g$ на рисунке (\ref{fig:one_neuron}) - это функция активации нейрона.  Цель использования функции активации заключается в том, чтобы позволить суммируемому результату меняться в зависимости от времени.Функцией по умолчанию является выпрямленная линейная активационная функция ReLU $g(x) = max(0,x)$,которая рекомендованна для использования с большинством нейронных сетей прямого распространения. Применение этой функции к выходу линейного преобразования приводит к нелинейному преобразованию. Однако функция остается очень близкой к линейной, в том смысле, что это кусочно-линейная функция с двумя линейными частями. Поскольку выпрямленные линейные единицы почти линейны, они сохраняют многие свойства, которые упрощают оптимизацию линейных моделей с помощью методов, основанных на градиенте. Другими популярными видами функции активации являются сигмоидальная(логистическая) функция $\sigma(x)=\frac{1}{1+e^{-x}}$, гиперболический тангенс $tanh(x) = \frac{e^x -e^{-x}}{e^x + e^{-x}}$.

Существует несколько видов обучения нейронных сетей:
\begin{itemize}
\item Обучение с учителем
\item Обучение без учителя
\end{itemize}

Подавляющее большинство искусственных нейронных сетевых решений проходят обучение с учителем. В этом режиме фактический выход нейронной сети сравнивается с желаемым выходом. Веса, которые обычно начинаются с произвольного начала, затем корректируются сетью, так что следующая итерация или цикл приведут к более близкому совпадению между желаемым и фактическим выходом. Метод обучения пытается минимизировать текущие ошибки всех элементов обработки. Это глобальное сокращение ошибок создается со временем, постоянно изменяя весы ввода до тех пор, пока не будет достигнута приемлемая точность сети.
При контролируемом обучении искусственная нейронная сеть должна быть обучена, прежде чем она станет полезна. Обучение состоит в представлении входных и выходных данных в сеть. Эти данные часто упоминаются как набор тренировок. То есть для каждого набора входных данных, предоставляемого системе, также предусмотрен соответствующий желаемый выходной набор. В большинстве приложений должны использоваться фактические данные. Эта стадия обучения может потреблять много времени. Затем используя метрики для расчета точности и качества модели, происходит процесс тренировки сети. Когда процесс тренировки заканчивается, то уже в online процессах используются эти натренированные параметры и веса. Некоторые типы сетей позволяют проводить непрерывную тренировку с гораздо меньшей скоростью во время работы. Это помогает сети адаптироваться к постепенно меняющимся условиям.

Сети без учителя не используют внешние воздействия для корректировки своих весов. Вместо этого они внутренне контролируют свою работу. Эти сети ищут закономерности или тенденции во входных сигналах и делают адаптацию в соответствии с функцией сети. Хотя и сеть обучается сама, необходимо специализировать, как сети огранизовать себя. Эта информация встроена в сетевую топологию и правила обучения.

Алгоритм обучения - алгорим обратного распространения ошибки, в котором используется стохастический градиентный спуск. Для задачи регресии чаще всего в качестве функции потерь используется средняя квадратичная ошибка(MSE)(\ref{loss_function}): 
\begin{equation}\label{loss_function}
L(y_{out},y_{true}) = \frac{1}{N}\sum_{i=1}^N(y_{out}(i) -y_{true}(i))^2
\end{equation}

Согласно универсальной теореме аппроксимации — нейронная сеть с одним скрытым слоем может аппроксимировать любую непрерывную функцию многих переменных с любой точностью. Главное чтобы в этой сети было достаточное количество нейронов. И еще важно удачно подобрать начальные значения весов нейронов. Чем удачнее будут подобраны веса, тем быстрее нейронная сеть будет сходиться к исходной функции. Это означает, что нелинейная характеристика нейрона может быть произвольной: от сигмоидальной до произвольного волнового пакета или вейвлета, синуса или многочлена. От выбора нелинейной функции может зависеть сложность конкретной сети, но с любой нелинейностью сеть остаётся универсальным аппроксиматором и при правильном выборе структуры может достаточно точно аппроксимировать функционирование любой непрерывной функции.

\section{Методы обучения c подкреплением}

Обучение с подкреплением (RL) - это область машинного обучения, связанная с тем, как агенты программного обеспечения должны предпринимать действия в среде, чтобы максимизировать некоторое понятие кумулятивной награды. В литературе по исследованиям и контролю операций обучение с подкреплением называется приближенным динамическим программированием или нейродинамическим программированием. Проблемы интереса к обучению подкреплению изучались также в теории оптимального управления, которая в основном связана с существованием и характеристикой оптимальных решений, алгоритмами их точного вычисления или аппроксимацией, особенно в отсутствие математической модели среды.

В машинном обучении среда обычно формулируется как процесс принятия решений Маркова (MDP), так как многие алгоритмы обучения c подкреплением для этого контекста используют методы динамического программирования. Основное различие между классическими методами динамического программирования и алгоритмами обучения с подкреплением заключается в том, что последние не предполагают знания точной математической модели MDP и нацеливаются на большие MDP, где точные методы становятся неосуществимыми. Компромисс между использованием наилучшей вычисленной стратегии и исследованием новых стратегий наиболее тщательно изучается в рамках многорукой бандитской проблемы и в конечных MDP.

Базовое RL моделируется как процесс принятия марковских решений:
\begin{itemize}
\item набор состояний среды и агента $S$
\item набор действий агента $A$
\item $P_{a}(s,s')=Pr(s_{t+1}=s'|s_{t}=s,a_{t}=a)$ - вероятность перехода из состояния $s$ в состояние $s'$ под действием агента $a$.
\item $R_ {a}(s, s')$ - непосредственная награда после перехода от $s$ к $s'$ с действием $a$.
\end{itemize}

Обучение с подкреплением требует механизмов исследования. Случайный выбор действий без ссылки на вероятное распределение вероятности показывает низкую производительность. Простые методы исследования являются наиболее практичными.

Одним из таких методов является $\epsilon$ -greedy, когда агент выбирает действие, которое, по его мнению, имеет лучший долгосрочный эффект с вероятностью $1- \epsilon$. Если никакое действие, удовлетворяющее этому условию, не найдено, агент выбирает действие равномерно случайным образом. Здесь $\epsilon <1$ является параметром настройки, который иногда изменяется либо по фиксированному расписанию, либо адаптивно основываясь на эвристике.

Выбор действия агента моделируется как таблица, называемая стратегией $\pi: S \times A \to [0,1]$:
\begin{equation}
\pi (a | s) = P (a_{t} = a | s_{t} = s)
\end{equation}
Таблица стратегии дает вероятность принятия действий $a$ в состоянии $s$. Существуют также невероятностные стратегии.

Функция стоимости $V_{\ pi}(s)$ определяется как ожидаемый возврат, начиная с состояния $s$, то есть $s_ {0} = s$ и последовательно следуют политике $\pi$. Следовательно, грубо говоря, функция значений оценивает «насколько хорошо» она должна находиться в определенном состоянии. 
\begin{equation}
V_{\pi}(s) = E[R] = E[\sum_{t = 0}^{\infty} \gamma^{t} r_{t} | s_{0} = s]
\end{equation}

где случайная величина $R$ обозначает прибыль, и определяется как сумма будущих дисконтированных вознаграждений.
\begin{equation}
R = \sum_{t = 0}^{\infty} \gamma ^ {t} r_ {t}
\end{equation}
где $r_ {t}$ - вознаграждение на этапе $t$,$\gamma \in [0,1]$ - ставка дисконтирования.

Функция стоимости пытается найти стратегию, которая максимизирует прибыль, поддерживая набор оценок ожидаемых результатов для некоторой стратегии (обычно либо «текущая» [внутри стратегии], либо оптимальная [вне стратегии]).

Эти методы основаны на теории MDP, где оптимальность определяется как: стратегия называется оптимальной, если она достигает наилучшей ожидаемой прибыли из любого начального состояния. 

Чтобы определить оптимальность формальным образом, определим прибыль от стратегии $\pi$
\begin{equation}
 V^{\pi} (s) = E[R | s, \pi ]
\end{equation}
где $R$ обозначает прибыль, связанную со следующей $\pi$ из исходного состояния $s$. Определим $V^{*}(s)$ как максимально возможное значение $V^{\pi}(s)$, где $\pi$ разрешено изменять,
\begin{equation}
V^{*}(s) = \max_{\pi} V^{\pi}(s)
\end{equation}
Стратегия, которая достигает этих оптимальных значений в каждом состоянии, называется оптимальной. 

Хотя значения состояний достаточно для определения оптимальности, полезно определить функцию прибыли от действия агентв. Учитывая состояние $s$, действие $a$ и политику $\pi$, значение действия пары $(s, a )$ от $\pi$ определяется формулой
\begin{equation}
Q^{\pi} (s, a) = E [R | s, a, \pi]
\end{equation}
где $R$ теперь обозначает случайную прибыль, связанную с первым действием $a$ в состоянии $s$ и последующим $\pi$.

Теория MDP утверждает, что если $\pi^*$ является оптимальной стратегией, мы действуем оптимально (принимаем оптимальное действие), выбирая действие из $Q^{\pi^{*}} (s, \cdot)$ с наивысшим значением в каждом состоянии, $s$. Функция прибыли от действия такой оптимальной стратегией $ Q^{\pi^{*}}$ называется оптимальной функцией прибыли от действия и является обычно обозначаемый $Q^{*}$. Таким образом, знание оптимальной функции стоимости действия достаточно, чтобы знать, как действовать оптимально.

Существует несколько алгоритмов для нахождения оптимальных стратегий: метод Монте-Карло, метод конечных разностей, метод прямого поиска стратегии. Каждый из которых имеет свои достоинства и недостатки, однако чаще всего используется стохастическая оптимизация и методы градиентного подъема.

\section{Методы машинного обучения в системах управления с прогнозирующей моделью}

Существует несколько подходов использования методов обучения в MPC системах:
\begin{itemize}
\item Аппроксимация закона управления 
\item Использование обучаемой модели для аппроксимации динамики прогнозирующей модели
\item Итерационный подход для построения терминального региона и функции из предыдущих итераций
\end{itemize}

\subsection{Аппроксимация закона управления}
\subsection{Аппроксимация динамики системы с прогнозирующей моделью}
\subsection{Итерационный подход}