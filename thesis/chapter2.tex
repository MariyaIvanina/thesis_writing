\chapter{Использование методов машинного обучения в системах управления с прогнозирующей моделью }

\section{Основные понятия нейронных сетей}

Нейронная сеть представляет собой серию алгоритмов, которые стремятся распознать базовые отношения в наборе данных посредством процесса, который имитирует работу человеческого мозга.

Нейронная сеть основана на наборе связанных единиц или узлов, называемых искусственными нейронами, которые свободно моделируют нейроны в биологическом мозге. Каждое соединение, подобно синапсам в биологическом мозге, может передавать сигнал от одного искусственного нейрона к другому. Искусственный нейрон, который получает сигнал, может обрабатывать его, а затем сигнализировать дополнительные искусственные нейроны, связанные с ним.

В общих реализациях нейронных сетей сигнал при соединении между искусственными нейронами является действительным числом, а выход каждого искусственного нейрона вычисляется некоторой нелинейной функцией от суммы его входов. Связи между искусственными нейронами называются ребрами. Искусственные нейроны и ребра обычно имеют вес, который регулируется по мере продолжения обучения. Вес увеличивает или уменьшает силу сигнала при соединении. Искусственные нейроны могут иметь такой порог, что сигнал посылается только тогда, когда совокупный сигнал пересекает этот порог. Как правило, искусственные нейроны агрегируются в слои. Различные слои могут выполнять различные виды преобразований на своих входах. Сигналы перемещаются от первого уровня (входного уровня) к последнему слою (выходному слою), возможно, после пересечения слоев несколько раз.

Ключевой моделью глубокого обучения являются нейронные сети с прямым распространением (многослойные персептроны). Целью данного вида нейронных сетей является аппроксимация некоторой функции $f^*$. Например, для классификатора $y = f^*(x)$ сеть отображает вход $x$ в категорию $y$. Сеть определяет отображение $y = f(x; \theta)$ и изучает значение параметров $\theta$, которые приводят к приближению функции наилучшим образом.

\begin{figure}[h]
\center{\includegraphics[scale=0.7]{neural_networks}}
\caption{Нейронная сеть со скрытыми слоями.}
\label{fig:nn}
\end{figure}

Нейронные сети называются сетями, потому что они обычно представляются объединением многих различных функций. Модель связана с ориентированным ациклическим графом (Рис. \ref{fig:nn}), описывающим, как функции состоят вместе. Например, мы могли бы иметь три функции $f^{(1)}$, $f^{(2)}$ и $f^{(3)}$, связанные в цепочке, с образованием $f(x) = f^{(3)}(f^{(2)}(f^{(1)}(x)))$. Эти цепные структуры являются наиболее часто используемыми структурами нейронных сетей. В этом случае $f^{(1)}$ называется первым слоем сети, $f^{(2)}$ называется вторым слоем и т. д. Длина цепочки слоев называется глубиной сети.

Каждый скрытый уровень сети обычно является векторным. Размерность этих скрытых слоев определяет ширину модели. Каждый элемент вектора может быть интерпретирован как играющий роль, аналогичную нейрону. Вместо того, чтобы думать о том, что слой представляет собой единую вектор-векторную функцию, мы также можем думать о том, что этот слой состоит из множества единиц, которые действуют параллельно, каждый из которых представляет собой вектор-скалярную функцию. Каждый блок напоминает нейрон в том смысле, что он получает вход от многих других единиц и вычисляет его собственное значение активации. 

\begin{figure}[h]
\center{\includegraphics[scale=0.7]{one_neuron}}
\caption{Строение нейрона.}
\label{fig:one_neuron}
\end{figure}

Нейрон обычно получает много одновременных входов. Каждый вход имеет свой собственный относительный вес, который дает входное воздействие, которое ему необходимо для функции суммирования элемента обработки. Эти веса выполняют тот же тип функции, что и различные синаптические силы биологических нейронов. В обоих случаях некоторые входы становятся более важными, чем другие, так что они оказывают большее влияние на обрабатывающий элемент, поскольку они объединяются для создания нейронного ответа. Веса - это адаптивные коэффициенты в сети, которые определяют интенсивность входного сигнала, зарегистрированного искусственным нейроном. Они являются мерой прочности соединения входа. Эти сильные стороны могут быть изменены в ответ на различные обучающие наборы и в соответствии с конкретной топологией сети или с помощью ее правил обучения.На рисунке (\ref{fig:one_neuron}) веса обозначены $w_i$, значения нейронов предыдущего слоя - $a_i$. $b$ параметр представляет собой смещение для линейного преобразования входных нейронов. Таким образом, мы получаем значение функции суммирования в виде линейного преобразования $z = b + \sum_{i=1}^{N}a_iw_i$.

Функция $g$ на рисунке (\ref{fig:one_neuron}) - это функция активации нейрона.  Цель использования функции активации заключается в том, чтобы позволить суммируемому результату меняться в зависимости от времени.Функцией по умолчанию является выпрямленная линейная активационная функция ReLU $g(x) = max(0,x)$,которая рекомендованна для использования с большинством нейронных сетей прямого распространения. Применение этой функции к выходу линейного преобразования приводит к нелинейному преобразованию. Однако функция остается очень близкой к линейной, в том смысле, что это кусочно-линейная функция с двумя линейными частями. Поскольку выпрямленные линейные единицы почти линейны, они сохраняют многие свойства, которые упрощают оптимизацию линейных моделей с помощью методов, основанных на градиенте. Другими популярными видами функции активации являются сигмоидальная(логистическая) функция $\sigma(x)=\frac{1}{1+e^{-x}}$, гиперболический тангенс $tanh(x) = \frac{e^x -e^{-x}}{e^x + e^{-x}}$.

Существует несколько видов обучения нейронных сетей:
\begin{itemize}
\item Обучение с учителем
\item Обучение без учителя
\end{itemize}

Подавляющее большинство искусственных нейронных сетевых решений проходят обучение с учителем. В этом режиме фактический выход нейронной сети сравнивается с желаемым выходом. Веса, которые обычно начинаются с произвольного начала, затем корректируются сетью, так что следующая итерация или цикл приведут к более близкому совпадению между желаемым и фактическим выходом. Метод обучения пытается минимизировать текущие ошибки всех элементов обработки. Это глобальное сокращение ошибок создается со временем, постоянно изменяя весы ввода до тех пор, пока не будет достигнута приемлемая точность сети.
При контролируемом обучении искусственная нейронная сеть должна быть обучена, прежде чем она станет полезна. Обучение состоит в представлении входных и выходных данных в сеть. Эти данные часто упоминаются как набор тренировок. То есть для каждого набора входных данных, предоставляемого системе, также предусмотрен соответствующий желаемый выходной набор. В большинстве приложений должны использоваться фактические данные. Эта стадия обучения может потреблять много времени. Затем используя метрики для расчета точности и качества модели, происходит процесс тренировки сети. Когда процесс тренировки заканчивается, то уже в online процессах используются эти натренированные параметры и веса. Некоторые типы сетей позволяют проводить непрерывную тренировку с гораздо меньшей скоростью во время работы. Это помогает сети адаптироваться к постепенно меняющимся условиям.

Сети без учителя не используют внешние воздействия для корректировки своих весов. Вместо этого они внутренне контролируют свою работу. Эти сети ищут закономерности или тенденции во входных сигналах и делают адаптацию в соответствии с функцией сети. Хотя и сеть обучается сама, необходимо специализировать, как сети огранизовать себя. Эта информация встроена в сетевую топологию и правила обучения.

Алгоритм обучения - алгорим обратного распространения ошибки, в котором используется стохастический градиентный спуск. Для задачи регресии чаще всего в качестве функции потерь используется средняя квадратичная ошибка(MSE)(\ref{loss_function}): 
\begin{equation}\label{loss_function}
L(y_{out},y_{true}) = \frac{1}{N}\sum_{i=1}^N(y_{out}(i) -y_{true}(i))^2
\end{equation}

Согласно универсальной теореме аппроксимации — нейронная сеть с одним скрытым слоем может аппроксимировать любую непрерывную функцию многих переменных с любой точностью. Главное чтобы в этой сети было достаточное количество нейронов. И еще важно удачно подобрать начальные значения весов нейронов. Чем удачнее будут подобраны веса, тем быстрее нейронная сеть будет сходиться к исходной функции. Это означает, что нелинейная характеристика нейрона может быть произвольной: от сигмоидальной до произвольного волнового пакета или вейвлета, синуса или многочлена. От выбора нелинейной функции может зависеть сложность конкретной сети, но с любой нелинейностью сеть остаётся универсальным аппроксиматором и при правильном выборе структуры может достаточно точно аппроксимировать функционирование любой непрерывной функции.

\section{Методы обучения c подкреплением}

Обучение с подкреплением (RL) - это область машинного обучения, связанная с тем, как агенты программного обеспечения должны предпринимать действия в среде, чтобы максимизировать некоторое понятие кумулятивной награды. В литературе по исследованиям и контролю операций обучение с подкреплением называется приближенным динамическим программированием или нейродинамическим программированием. Проблемы интереса к обучению подкреплению изучались также в теории оптимального управления, которая в основном связана с существованием и характеристикой оптимальных решений, алгоритмами их точного вычисления или аппроксимацией, особенно в отсутствие математической модели среды.

В машинном обучении среда обычно формулируется как процесс принятия решений Маркова (MDP), так как многие алгоритмы обучения c подкреплением для этого контекста используют методы динамического программирования. Основное различие между классическими методами динамического программирования и алгоритмами обучения с подкреплением заключается в том, что последние не предполагают знания точной математической модели MDP и нацеливаются на большие MDP, где точные методы становятся неосуществимыми. Компромисс между использованием наилучшей вычисленной стратегии и исследованием новых стратегий наиболее тщательно изучается в рамках многорукой бандитской проблемы и в конечных MDP.

Базовое RL моделируется как процесс принятия марковских решений:
\begin{itemize}
\item набор состояний среды и агента $S$
\item набор действий агента $A$
\item $P_{a}(s,s')=Pr(s_{t+1}=s'|s_{t}=s,a_{t}=a)$ - вероятность перехода из состояния $s$ в состояние $s'$ под действием агента $a$.
\item $R_ {a}(s, s')$ - непосредственная награда после перехода от $s$ к $s'$ с действием $a$.
\end{itemize}

Обучение с подкреплением требует механизмов исследования. Случайный выбор действий без ссылки на вероятное распределение вероятности показывает низкую производительность. Простые методы исследования являются наиболее практичными.

Одним из таких методов является $\epsilon$ -greedy, когда агент выбирает действие, которое, по его мнению, имеет лучший долгосрочный эффект с вероятностью $1- \epsilon$. Если никакое действие, удовлетворяющее этому условию, не найдено, агент выбирает действие равномерно случайным образом. Здесь $\epsilon <1$ является параметром настройки, который иногда изменяется либо по фиксированному расписанию, либо адаптивно основываясь на эвристике.

Выбор действия агента моделируется как таблица, называемая стратегией $\pi: S \times A \to [0,1]$:
\begin{equation}
\pi (a | s) = P (a_{t} = a | s_{t} = s)
\end{equation}
Таблица стратегии дает вероятность принятия действий $a$ в состоянии $s$. Существуют также невероятностные стратегии.

Функция стоимости $V_{\ pi}(s)$ определяется как ожидаемый возврат, начиная с состояния $s$, то есть $s_ {0} = s$ и последовательно следуют политике $\pi$. Следовательно, грубо говоря, функция значений оценивает «насколько хорошо» она должна находиться в определенном состоянии. 
\begin{equation}
V_{\pi}(s) = E[R] = E[\sum_{t = 0}^{\infty} \gamma^{t} r_{t} | s_{0} = s]
\end{equation}

где случайная величина $R$ обозначает прибыль, и определяется как сумма будущих дисконтированных вознаграждений.
\begin{equation}
R = \sum_{t = 0}^{\infty} \gamma ^ {t} r_ {t}
\end{equation}
где $r_ {t}$ - вознаграждение на этапе $t$,$\gamma \in [0,1]$ - ставка дисконтирования.

Функция стоимости пытается найти стратегию, которая максимизирует прибыль, поддерживая набор оценок ожидаемых результатов для некоторой стратегии (обычно либо «текущая» [внутри стратегии], либо оптимальная [вне стратегии]).

Эти методы основаны на теории MDP, где оптимальность определяется как: стратегия называется оптимальной, если она достигает наилучшей ожидаемой прибыли из любого начального состояния. 

Чтобы определить оптимальность формальным образом, определим прибыль от стратегии $\pi$
\begin{equation}
 V^{\pi} (s) = E[R | s, \pi ]
\end{equation}
где $R$ обозначает прибыль, связанную со следующей $\pi$ из исходного состояния $s$. Определим $V^{*}(s)$ как максимально возможное значение $V^{\pi}(s)$, где $\pi$ разрешено изменять,
\begin{equation}
V^{*}(s) = \max_{\pi} V^{\pi}(s)
\end{equation}
Стратегия, которая достигает этих оптимальных значений в каждом состоянии, называется оптимальной. 

Хотя значения состояний достаточно для определения оптимальности, полезно определить функцию прибыли от действия агентв. Учитывая состояние $s$, действие $a$ и политику $\pi$, значение действия пары $(s, a )$ от $\pi$ определяется формулой
\begin{equation}
Q^{\pi} (s, a) = E [R | s, a, \pi]
\end{equation}
где $R$ теперь обозначает случайную прибыль, связанную с первым действием $a$ в состоянии $s$ и последующим $\pi$.

Теория MDP утверждает, что если $\pi^*$ является оптимальной стратегией, мы действуем оптимально (принимаем оптимальное действие), выбирая действие из $Q^{\pi^{*}} (s, \cdot)$ с наивысшим значением в каждом состоянии, $s$. Функция прибыли от действия такой оптимальной стратегией $ Q^{\pi^{*}}$ называется оптимальной функцией прибыли от действия и является обычно обозначаемый $Q^{*}$. Таким образом, знание оптимальной функции стоимости действия достаточно, чтобы знать, как действовать оптимально.

Существует несколько алгоритмов для нахождения оптимальных стратегий: метод Монте-Карло, метод конечных разностей, метод прямого поиска стратегии. Каждый из которых имеет свои достоинства и недостатки, однако чаще всего используется стохастическая оптимизация и методы градиентного подъема.

\section{Методы дизайна оффлайн регуляторов в системах управления с прогнозирующей моделью}

Существует несколько подходов использования методов обучения в MPC системах:
\begin{itemize}
\item Аппроксимация закона управления 
\item Использование обучаемой модели для аппроксимации динамики прогнозирующей модели
\item Итерационный подход для построения терминального региона и функции из предыдущих итераций
\end{itemize}

\subsection{Аппроксимация закона управления}
Для линейных систем задача оптимизации может быть решена оффлайн, т.е. до онлайн запуска системы при некоторых слабых предположениях \cite{Bemporad}. Таким образом, получается явный закон управления. Расширение \cite{Bemporad} до нелинейных систем не является прямым, и существуют также проблемы сложности вычислений, касающиеся метода из \cite{Bemporad}.

Система вида $x^+=Ax+Bu$ с линейными ограничениями вида
$C_xx \leq d_x$,  $C_uu \leq d_u$, где оптимизационная задача записывается в таком виде: 

В момент времени $t$, вычисляем состояние $x(t)$, решаем задачу
\begin{equation*}
\min_{u(\cdot|t)}J(x,u) = \sum_{k=t}^{t+N-1}L(x(k|t),u(k|t)) + F(x(t+N|t))
\end{equation*}
с условиями
\begin{equation*}
x(k+1|t) = Ax(k|t) + Bu(k|t)
\end{equation*}
\begin{equation*}
x(t|t) = x(t)
\end{equation*}
\begin{equation*}
C_xx(k|t) \leq d_x
\end{equation*}
\begin{equation*}
C_uu(k|t) \leq d_u
\end{equation*}
for $ t \leq k \leq t+N-1$ и терминальным ораничением
\begin{equation*}
C^fx(t+N|t) \leq d^f
\end{equation*}
с квадратичной функцией стоимости перехода и квадратичной терминальной функцией $L(x,u) = x^TQx + u^TRu, \ Q,R > 0, \ F(x) = x^TPx$

Можно переписать задачу в явном виде, обзначим
\begin{equation}
X:= [x^T(t+1|t), \dots, x^T(t+N|t)]^T
\end{equation}
\begin{equation}
U:= [u^T(t+1|t), \dots, u^T(t+N-1|t)]^T
\end{equation}
\begin{itemize}
	\item Перепишем функцию стоимости
	\begin{equation}\label{cost_function}
	F(x(t),U) = x^T(t)Qx(t) + X^T\tilde{Q}X + U^T\tilde{R}U
	\end{equation}
	with $\tilde{Q} = 
	\begin{bmatrix}
    Q  & \  & \ \\
    \  &\ddots & \\
     \  & \ & Q \\
     \ & \ & \ & P     
	\end{bmatrix} $, 
	$\tilde{R} = 
	\begin{bmatrix}
    R  & \  & \ \\
    \  &\ddots & \\
     \  & \ & R     
	\end{bmatrix} $
	\item Перепишем динамику системы: $x(t+k|t) = A^kx(t) + 		\sum_{j=0}^{k-1}A^jBu(t+k-j-1|t), \ k=1,..,N$
	\begin{equation}\label{rewritten_state_input}
\Rightarrow X=\begin{bmatrix}
   A \\
   A^2 \\
   \vdots \\
   A^N   
\end{bmatrix}x(t) +\begin{bmatrix}
    B  & 0 & \dots &  & 0 \\
    AB & B & 0 & \dots & 0\\
    \hdots & & & & \\
    A^{N-1}B  & A^{N-2}B & \dots & \dots & B     
\end{bmatrix}U
\end{equation}
где $\Omega = \begin{bmatrix}
   A \\
   A^2 \\
   \vdots \\
   A^N   
\end{bmatrix}$ и $\Gamma = \begin{bmatrix}
    B  & 0 & \dots &  & 0 \\
    AB & B & 0 & \dots & 0\\
    \hdots & & & & \\
    A^{N-1}B  & A^{N-2}B & \dots & \dots & B     
\end{bmatrix}$
\end{itemize}

Подставим (\ref{rewritten_state_input}) в (\ref{cost_function}) : $J(x(t),U) = \frac{1}{2}x^T(t)Yx(t) + \frac{1}{2}U^THU + x^T(t)FU$
\begin{equation*}
Y = 2(Q + \Omega^T \tilde{Q} \Omega)
\end{equation*}
\begin{equation*}
H = 2(\Gamma^T\tilde{Q}\Gamma + \tilde{R})
\end{equation*}
\begin{equation*}
F = 2\Omega^T \tilde{Q} \Gamma
\end{equation*}

Ограничения можно переписать таким же образом и получим 
\begin{equation*}
GU \leq W + Ex(t)
\end{equation*}

\begin{equation}\label{standard_problem}
\min_U \frac{1}{2} U^THU + x^TFU + \frac{1}{2}x^TYx
\end{equation}
при условии $GU \leq W + Ex$

Далее еще применим замену переменных вида $z:=U+H^{-1}F^Tx$,где $H^{-1}$ - положительно определенная матрица. 

\begin{equation}\label{rewritten_problem}
\min_{z} \frac{1}{2}z^THz + \frac{1}{2}x^T \tilde{Y}x
\end{equation}
s.t. $Gz \leq W + Sx$
\begin{equation*}
\tilde{Y} := Y - FH^{-1}F^T
\end{equation*}
\begin{equation*}
S := E + GH^{-1}F^T
\end{equation*}

Далее эта задача решается посредством выпуклой оптимизации через условия Каруша-Куна-Такера и тогда так как задача (строго)выпуклая с разрешим множеством с непустой внутренней частью(по предположениям), значит условия Слейтера выполняются. Оптимальное решение единственное и характеризуется условиями KKT. 

Оптимальное множество $z^*(x)$ - оптимальное решение задачи (\ref{rewritten_problem}) для данного $x$
\begin{equation*}
A(x) := \{ i \in \{1, \dots q\} | G^iz^*(x) = W^i + S^ix \}
\end{equation*}

$G^A, \ W^A, \ S^A \dots$ матрицы, содержашие ряды $G, \ U, \ S$, которые ассоциируются с множеством $A$. 

Тогда мы имеем решение $\lambda^A$ и $z^*(x)$
\begin{equation}
\lambda^A = - (G^AH^{-1}(G^A)^T)^{-1}(W^A+S^Ax)
\end{equation} 
\begin{equation}
z^*(x) = H^{-1}(G^A)^T(G^AH^{-1}(G^A)^T)^{-1}(W^A + S^Ax)
\end{equation}

Множество состояний, где $A$ - оптимальное активное множество(критический регион $CR^A$). Критический регион задается посредством следующих неравенств:

\begin{equation*}\label{inequality_with_z}
\left \{
  \begin{tabular}{l}
  $GH^{-1}(G^A)^T(G^AH^{-1}(G^A)^T)^{-1}(W^A+S^Ax) \leq W + Sx$ \\
  $-(G^AH^{-1}(G^A)^T)^{-1}(W^A+S^Ax) \geq 0$
  \end{tabular}
 \right.
\end{equation*}

\begin{theorem}
Для линейных MPC(линейной системы, линейных ограничений, квадритичной стоимости перехода), результирующий MPC положительно определенный регулятор 
$u_{MPC}(x)$ непрерывный и кусочно-афинный на регионах в виде многогранника.Оптимальное значение функции стоимости для задачи (\ref{standard_problem}), $F^*(x)$, непрерывное, выпуклое и кусочно-квадратичное.
\end{theorem}
Явный MPC решает задачу для всех состояний, таким образом все пространство состояний делится на регионы, где в каждом регионе для состояния есть явная функция управления. Главный недостаток этого метода состоит в том, что количество регионов может быть достаточно большим, что в онлайн процедуре может плохо сказываться на производительности, т.к. нужно будет искать к какому из регионов относится текущее состояние,чтобы опредилить управление для него.

Cуществует несколько подходов к получению аппроксимативного решения для оптимизации MPC. Для линейных систем в \cite{Domahidi} алгоритм обучения представлен дополнительными ограничениями для обеспечения стабильности и ограничения удовлетворения аппроксимационного MPC. Одним из подходов к аппроксимации MPC является выпуклое многопараметрическое нелинейное программирование \cite{Johansen}, где вычисляется субоптимальная аппроксимация закона управления MPC. Другой подход - аппроксимировать MPC с помощью методов машинного обучения. Это делают нейронные сети в \cite{ParisiniZoppoli}, \cite{ParisiniSanquineti}, \cite{Akesson}. Эти методы не гарантируют устойчивость или удовлетворение ограничениям для аппроксимационного MPC, что особенно важно, если рассматривать жесткие ограничения на состояния. В \cite{Chakrabarty} используется метод опорных векторов (SVM) для аппроксимации MPC. Устойчивость и удовлетворение ограничений могут быть гарантированы для произвольных ошибок малого приближения, основанных на присущих свойствам устойчивости. В \cite{Pin} аппроксимируется MPC с липшицевым сужающим ограничением, что обеспечивает устойчивость при неисчезащих ошибках аппроксимации. Ошибка аппроксимации, выведенная в \cite{Chakrabarty}, \cite{Pin}, обычно не достижима для практического применения.

B \cite{ParisiniSanquineti} хотят найти аппроксимирующий закон управления $u_t^{RH^0}=\gamma^0_{RH}(x_t) \in U$, где $u_t^{RH^0}$ - первый вектор последовательности управления, которая минимизирует стоимость 
\begin{equation*}
J_{FH}(x_t,u_{t,t+N-1},N,a,P) = \sum_{i=t}^{t+N-1}l(x_i,u_i) + a \|x_{t+N}\|^2_P, \ t \geq 0
\end{equation*}

Стоимость формируется из стоимости переходов на горизонте планирования длины $N$ и терминальной функции. Аппроксимация закона управления происходит с помощью нейронной сети: $m$  параллельных сетей с одним выходным параметром,состоящий из одного скрытого слоем с $v_j$ нейронами на скрытом слое для сети $j=1..m$ и линейными активационными функциями. 

Для каждой функции $\hat{\gamma}^{(v_j)}_{RH_j}$ нужно найти количество нейронов $v_1^*, ..., v^*_m$, такое что 
\begin{equation}\label{neuron_inequality}
\min_{w_j}\max_{x_t \in X}|\gamma^0_{RH_j}(x_t) - \gamma^{(v_j)}_{RH_j}(x_t,w_j)| \leq \frac{\epsilon}{\sqrt{m}}, \ j = 1, ..., m
\end{equation}

Процедура нахождения количества нейронов достаточно наивная, но работающая: для каждого $j$ увеличиваем $v_j$ пока (\ref{neuron_inequality}) не станет верным. Также придовится в статье теорема, в которой утверждается, что для любой функции управления $\gamma^0_{RH_j}(x_t)$ число параметров, необходимых для достижения погрешности приближения $L_2$ или $L_{\infty}$ порядка $O(\frac{1}{v_j})$, равно $O(v_jn)$, которое растет линейно с размерностью $n$ вектора состояния.  

В \cite{Akesson} рассматривалась система нелинейного MPC с управляемым выходом:
\begin{equation*}
\left \{
  \begin{tabular}{l}
  $x(k+1) = f(x(k),u(k))$ \\
  $y(k) = h(x(k))$
  \end{tabular}
 \right.
\end{equation*}

И для этой системы минимизировалась стоимость вида:
\begin{equation}\label{min_func_neuron}
\begin{split}
J_N(U_N(k);k) = \sum_{i=k}^{k+N-1} [(\hat{y}(i+1|k) - y_r(i+1))^TQ(\hat{y}(i+1|k)-y_r(+1)) \\ 
      + \triangle u(i)^T R \triangle u(i)] + q_N(\hat{x}(k+N|k))
\end{split}
\end{equation}
где $y_r$ - эталонная траектория, $\triangle u(k) = u(k) - u(k-1)$ - инкрементное управление. Для этой задачи создают нейронный регулятор вида $\triangle u_{opt}(k) = g(I_{MPC}(k))$, где $I_{MPC}(k) = \{\hat{x}(k|k), y_r(k+1), ..., y_r(k+N)\}$. Данный нейронный регулятор обучается с функцией потерь 
вида (\ref{min_func_neuron}). Для решения оптимизационной задачи на этапе обучения нейронной сети используется градиентный спуск. Достаточно хорошие результаты на вычислительных этапах были получены, однако опять же не гарантируется устойчивость данного метода.

В работе \cite{Pin} уже вводятся некоторые гарантии устойчивости метода.

\subsection{Аппроксимация динамики системы с прогнозирующей моделью}

Существует методы для аппроксимации динамики системы, в работе \cite{Aswani} был предложен доказуемо безопасный и робастный метод управления основанный на обучении для систем с прогнозирующей моделью. Мотивация этой статьи состоит в том, чтобы разработать схему управления, которая может
\begin{enumerate}
\item обрабатывать ограничения состояния и управления
\item оптимизировать производительность системы в отношении функции стоимости
\item использовать инструменты статистической идентификации для изучения неопределенностей модели
\item доказуемо сходиться
\end{enumerate}

Введена форма надежного, адаптивного модельного прогнозирующего управления, основанный на обучении метод, на который ссылаются, как LBMPC. Основная идея LBMPC заключается в том, что производительность и безопасность могут быть разделены в рамках MPC с использованием инструментов доступности. В частности, LBMPC повышает производительность за счет выбора входных данных, которые минимизируют затраты, связанные с динамикой изученной модели, которая обновляется с использованием статистики, обеспечивая при этом безопасность и стабильность, используя теорию из робастного MPC, чтобы проверить, примененное управление сохраняет ли номинальную модель устойчивой, когда она подвержена неопределенности.

Динамика системы представляется в таком виде:
\begin{equation*}
x_{n+1} = Ax_n + Bu_n + g(x_n, u_n)
\end{equation*}
где $g(x,u)$ описывает несмоделированную динамику, которая по предположению ограничена и лежит в политопе $W$. 

Данный метод вводит дополнительную систему, которая обучается на данных и имеет такой вид:
\begin{equation*}
\tilde x_{n+1} = A \tilde x_n + B \tilde u_n + O_n(\tilde x_n, \tilde u_n)
\end{equation*}
где $O_n$ - зависящая от времени функция, которая обучается с помощью любого из статистических метод, параметрических или нет. 

Вся теория устойчивости строится на том, что наша система представляется в виде робастного MPC с обученой динамикой на известных вычисленных точках, и может адаптироваться к новым полученным точкам, чтобы улучшать точность обученной динамики системы. 

Задача уже формулируется в таком виде:
\begin{equation*}
V_n(x_n) = \min_{c, \theta} \phi_n(\theta, \tilde{x_n}, ..., \tilde{x_{n+N}}, \tilde{u}_n, ..., \tilde{u}_{n+N-1})
\end{equation*}
при условиях
\begin{equation*}
\tilde{x_n}=x_n, \ \bar{x_n} = x_n
\end{equation*}
\begin{equation*}
\tilde x_{n+i+1} = A \tilde x_{n+i} + B \tilde u_{n+i} + O_n(\tilde x_{n+i}, \tilde u_{n+i})
\end{equation*}
\begin{equation*}
\bar{x_{n+i+1}}=A\bar{x_{n+i}}+B\tilde{u_{n+i}}
\end{equation*}
\begin{equation*}
\tilde{u_{n+i}} = K\bar{x_{n+i}}+c_{n+i}
\end{equation*}
\begin{equation*}
\bar{x_{n+i+1}} \in X \ominus R_i, \ \tilde{u_{n+i}} \in U \ominus KR_i
\end{equation*}
\begin{equation*}
(\bar{x_{n+N,\theta}}) \in \Omega \ominus (R_N \times \{0\})
\end{equation*}
где $\Omega$ - допустимое инвариантное робастное множество таких точек, что любая траектория системы с начальным условием, выбранным из этого множества и с управлением $u_n$, остается в множестве для любой последовательности ограниченного возмущения, удовлетворяя ограничениям на состояние и управление. И тогда с помощью данного неравенства формулируется удовлетворение ограничений:
\begin{equation*}
\Omega \subseteq \{ (\bar{x}, \theta): \bar{x} \in X; \Lambda \theta \in X; K \bar{x} + (\Psi - K \Lambda)\theta \in U; \Psi \theta \in U\}
\end{equation*}

А с помощью данного неравенства инвариантность возмущения:
\begin{equation*}
\begin{bmatrix}
   A+BK & B(\Psi - K\Lambda) \\
   0 &  \mathbb{I} 
\end{bmatrix}\Omega \oplus (W \times \{0\}) \subseteq \Omega
\end{equation*}

Так как $\bar{x_s} = \Lambda \theta$ и $\bar{u_s} = \Psi \theta$, где $\theta \in \mathbb{R}^m$ и $\Lambda \in \mathbb{R}^{n \times m}, \ \Psi \in \mathbb{R}^{m \times m}$ - точки устойчивого состояния, то их можно достичь для системы если $A+BK$ устойчива по Шуру при управлении $\bar{u_n} = K(\bar{x_n} - \bar{x_s}) + \bar{u_s} = K\bar{x_n} + (\Psi - K\Lambda)\theta$.Множества $R_0 = \{0\}$ и $R_i = \oplus_{j=0}^{i-1}(A+BK)^jW$ необходимы для робастного MPC, сужающиеся ограничения. 

Устойчивость данного метода основана на робастном MPC.Результаты экспериментов показали хорошие результаты, были получены хорошие результаты относительно предсказанных траекторий и быстрой сходимости к точке устойчивого состояния, однако по производительности уступало нелинейному MPC.

\subsection{Итерационный подход}

Существует итерационный подход для построения MPC регулятора \cite{Rosolia}. Регулятор имеет справочную информацию и способен улучшать свою производительность, изучая предыдущие итерации. Для обеспечения рекурсивной выполнимости и неубывающей производительности на каждой итерации используются безопасное терминальное множество и терминальная функция стоимости. Построение данного регулятора обеспечивает тот факт, что функция стоимости убывает с каждой итерацией, также из удовлетворения ограничений на $j-1$ итерации следует удовлетворение ограничений на $j$ итерации и точка равновесия замкнутой системы асимптотически устойчива. Оптимальность траекторий построенных этим регулятором доказывается для выпуклых задач.

\begin{figure}[h]
\center{\includegraphics[scale=0.7]{iterative}}
\caption{Построение сэмплированного безопасного множества.}
\label{fig:sample_safe_set}
\end{figure}

Управляемое на $N$ шагов вперед множество по отношению к $S$ формулируется рекурсивно как:
\begin{equation}
K_j(S) = Pre(K_{j-1}(S)) \cap X, \ K_0(S) = S, j \in \{1, ..., N\}
\end{equation}
где $Pre(S) = \{x \in \mathbb{R}^n: \exists u \in U s.t.\ f(x,u) \in S\}$

Сэмплированное безопасное множество на итерации $j$
\begin{equation*}
SS^j = \{\cup_{i \in M^j} \cup_{t=0}^{\infty} x_t^i\}
\end{equation*}
где $M^j = \{k \in [0,j]: \lim_{t \to \infty}x^k_t = x_F \}$. $SS^j$ это множество всех траекторий на итерации $i$ для $i \in M^j$. Как выглядит это множество показано на рисунке \ref{fig:sample_safe_set}.

Вводится также стоимость на этом сэмплированном безопасном множестве:
\begin{equation*}
Q^j(x) = \left \{
  \begin{tabular}{l}
  $\min_{(i,t) \in F^j(x)} J^i_{t \to \infty}, \ x\in SS^j$ \\
  $+ \infty, \ x \not\in SS^j$
  \end{tabular}
 \right.
\end{equation*}

Тогда формулировка MPC функции стоимости,которую необходимо минимизировать будет звучать так:
\begin{equation*}
J^{LMPC,j}_{t \to t+N}(x_t^j) = \min_{u_{t|t},..., u_{t+N-1|t}}[\sum_{k=t}^{t+N-1}h(x_{k|t},u_{k|t}) + Q^{j-1}(x_{t+N|t})]
\end{equation*}
при условии
\begin{equation*}
x_{k+1|t} = f(x_{k|t}, u_{k|t}) \ \forall k \in [t, .., t+N-1]
\end{equation*}
\begin{equation*}
x_{k|t} \in X, \ u_{k|t} \in U, \ \forall k \in [t,..,t+N-1]
\end{equation*}
\begin{equation*}
x_{t+N|t} \in SS^{j-1}
\end{equation*}
\begin{equation*}
x_{t|t} = x_t^j
\end{equation*}

Данный метод обладает рекурсивной выполнимостью, устойчивостью и сходимостью. Но предлагаемый подход является дорогостоящим с точки зрения вычисления даже для линейной системы, поскольку регулятор должен решать задачу смешанного целочисленного программирования в каждый момент времени. Есть улучшения данного подхода с использованием параллельных вычислений, а также попытки сделать терминальные ограничения более выпуклыми.